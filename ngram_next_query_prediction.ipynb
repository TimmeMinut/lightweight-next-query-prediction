{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def preprocess_aol_query_log(input_dir):\n",
    "\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    # Regular expression to find queries consisting of only special characters\n",
    "    special_chars_only_pattern = re.compile(r'^[_\\W\\s]*$')\n",
    "\n",
    "    total_processed_lines = 0\n",
    "    total_duplicates_removed = 0\n",
    "    total_special_chars_removed = 0\n",
    "    total_malformed_ids_removed = 0\n",
    "    total_malformed_lines_skipped = 0\n",
    "    total_empty_lines_skipped = 0\n",
    "    total_files = 0\n",
    "\n",
    "    # Sets to track userIDs\n",
    "    total_userids = set()\n",
    "    remaining_userids = set()\n",
    "\n",
    "    # Create output directory and sub-directory for processed files\n",
    "    output_dir = f\"aol_processed\"\n",
    "    processed_dir = os.path.join(output_dir, \"processed_files\")\n",
    "    if not os.path.exists(processed_dir):\n",
    "        os.makedirs(processed_dir)\n",
    "\n",
    "    # Create filepaths for output files\n",
    "    special_char_output_file = os.path.join(\n",
    "        output_dir, \"special_char_queries.txt\")\n",
    "    malformed_id_output_file = os.path.join(\n",
    "        output_dir, \"malformed_id_queries.txt\")\n",
    "    stats_output_file = os.path.join(output_dir, \"processing_stats.txt\")\n",
    "\n",
    "    # Open miscellaneous output files for writing\n",
    "    with open(special_char_output_file, 'w', encoding='utf-8') as special_char_file, \\\n",
    "            open(malformed_id_output_file, 'w', encoding='utf-8') as malformed_id_file, \\\n",
    "            open(stats_output_file, 'w', encoding='utf-8') as stats_file:\n",
    "\n",
    "        print(f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
    "        stats_file.write(\n",
    "            f\"AOL Query Log Processing - Started at {timestamp}\\n\")\n",
    "\n",
    "        for filename in os.listdir(input_dir):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            total_files += 1\n",
    "\n",
    "            # Create pathnames for input file and output file\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(processed_dir, filename)\n",
    "\n",
    "            print(f\"Processing file {filename}\")\n",
    "            stats_file.write(f\"Processing file {filename}\\n\")\n",
    "\n",
    "            file_processed_lines = 0\n",
    "            file_duplicates_removed = 0\n",
    "            file_special_chars_removed = 0\n",
    "            file_malformed_ids_removed = 0\n",
    "            file_malformed_lines_skipped = 0\n",
    "            file_empty_lines_skipped = 0\n",
    "\n",
    "            # Open input file and create output file\n",
    "            try:\n",
    "                with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "                    next(infile)  # Skip the header\n",
    "\n",
    "                    prev_anon_id = None\n",
    "                    prev_query = None\n",
    "\n",
    "                    for line in infile:\n",
    "                        file_processed_lines += 1\n",
    "\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            file_empty_lines_skipped += 1\n",
    "                            continue  # Skip empty lines\n",
    "\n",
    "                        parts = line.split('\\t')\n",
    "                        if len(parts) < 3:\n",
    "                            file_malformed_lines_skipped += 1\n",
    "                            continue  # Skip malformed lines\n",
    "\n",
    "                        anon_id = parts[0].strip()\n",
    "                        query = parts[1].strip()\n",
    "\n",
    "                        if not anon_id.isdigit():\n",
    "                            file_malformed_ids_removed += 1\n",
    "                            malformed_id_file.write(\n",
    "                                f\"{line}\\t{filename}\\n\")\n",
    "                            # Skip malformed anonIDs\n",
    "                            continue\n",
    "\n",
    "                        total_userids.add(anon_id)\n",
    "\n",
    "                        is_duplicate = (\n",
    "                            anon_id == prev_anon_id and query == prev_query)\n",
    "\n",
    "                        is_special_chars_only = bool(\n",
    "                            special_chars_only_pattern.match(query))\n",
    "\n",
    "                        if is_duplicate:\n",
    "                            file_duplicates_removed += 1\n",
    "                        elif is_special_chars_only:\n",
    "                            file_special_chars_removed += 1\n",
    "                            special_char_file.write(line + '\\n')\n",
    "                        else:\n",
    "                            # Only keep a query if its unique and not only consisting of special characters.\n",
    "                            # Modification: Since some rows have 3 columns of data and others 5,\n",
    "                            # we remove the columns for ClickURL and ItemRank so that pandas can create a dataframe\n",
    "                            # from the input data, and since we don't use them anyway.\n",
    "                            outfile.write(anon_id + \"\\t\" + query + '\\n')\n",
    "                            remaining_userids.add(anon_id)\n",
    "\n",
    "                        prev_anon_id = anon_id\n",
    "                        prev_query = query\n",
    "\n",
    "                    total_processed_lines += file_processed_lines\n",
    "                    total_duplicates_removed += file_duplicates_removed\n",
    "                    total_special_chars_removed += file_special_chars_removed\n",
    "                    total_malformed_ids_removed += file_malformed_ids_removed\n",
    "                    total_malformed_lines_skipped += file_malformed_lines_skipped\n",
    "                    total_empty_lines_skipped += file_empty_lines_skipped\n",
    "\n",
    "                    file_stats = [\n",
    "                        f\"  - Processed: {file_processed_lines:,} queries\",\n",
    "                        f\"  - Skipped {file_empty_lines_skipped:,} empty lines\",\n",
    "                        f\"  - Skipped {file_malformed_lines_skipped:,} malformed lines\",\n",
    "                        f\"  - Removed {file_malformed_ids_removed:,} queries with malformed IDs\",\n",
    "                        f\"  - Removed {file_duplicates_removed:,} duplicate queries\",\n",
    "                        f\"  - Removed {file_special_chars_removed:,} special-character-only queries\",\n",
    "                        f\"  - Remaining: {file_processed_lines - file_duplicates_removed - file_special_chars_removed - file_malformed_ids_removed:,} queries\\n\"\n",
    "                    ]\n",
    "\n",
    "                    # Print and write the file stats\n",
    "                    for stat in file_stats:\n",
    "                        print(stat)\n",
    "                        stats_file.write(stat + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error processing {filename}: {str(e)}\"\n",
    "                print(error_msg)\n",
    "                stats_file.write(error_msg + \"\\n\")\n",
    "\n",
    "        remaining = total_processed_lines - total_duplicates_removed - \\\n",
    "            total_special_chars_removed - total_malformed_ids_removed\n",
    "\n",
    "        summary_stats = [\n",
    "            \"\\n\" + \"=\"*50,\n",
    "            \"PROCESSING COMPLETE\",\n",
    "            \"=\"*50,\n",
    "            f\"Processed {total_files} files with {total_processed_lines:,} total queries\",\n",
    "            f\"Skipped {total_empty_lines_skipped:,} empty lines\",\n",
    "            f\"Skipped {total_malformed_lines_skipped:,} malformed lines\",\n",
    "            f\"Removed {total_malformed_ids_removed:,} queries with malformed IDs\",\n",
    "            f\"Removed {total_duplicates_removed:,} duplicate queries ({total_duplicates_removed/total_processed_lines*100:.2f}%)\",\n",
    "            f\"Removed {total_special_chars_removed:,} special-char queries ({total_special_chars_removed/total_processed_lines*100:.2f}%)\",\n",
    "            f\"Remaining non-duplicate, valid ID queries: {total_processed_lines - total_duplicates_removed - total_malformed_ids_removed:,} ({(total_processed_lines - total_duplicates_removed - total_malformed_ids_removed)/total_processed_lines*100:.2f}%)\",\n",
    "            f\"Remaining non-duplicate, valid ID, non-special-char queries: {remaining:,} ({remaining/total_processed_lines*100:.2f}%)\",\n",
    "            f\"Removed userIDs: {len(total_userids) - len(remaining_userids)}\",\n",
    "            f\"Remaining UserIDs after processing: {len(remaining_userids)}\",\n",
    "            \"=\"*50,\n",
    "        ]\n",
    "\n",
    "        for stat in summary_stats:\n",
    "            print(stat)\n",
    "            stats_file.write(stat + \"\\n\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        stats_file.write(\n",
    "            f\"\\nProcessing completed at {datetime.datetime.now()}\\n\")\n",
    "        stats_file.write(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Preprocess AOL query log files\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"input_dir\", help=\"Path to input directory containing AOL query log files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    preprocess_aol_query_log(args.input_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import kenlm\n",
    "\n",
    "# === Paths ===\n",
    "BASE_PATH = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/aol_processed/\"\n",
    "INPUT_DIR = BASE_PATH + \"processed_files/\"\n",
    "TOKENIZED_FILE_PATH = BASE_PATH + \"queries_tokenized.txt\"\n",
    "VOCAB_DICT_PATH = BASE_PATH + \"vocab_dict.json\"\n",
    "VOCAB_STATS_PATH = BASE_PATH + \"vocab_stats.json\"\n",
    "\n",
    "\n",
    "def tokenize_query(query):\n",
    "    \n",
    "    # Treat '.' as a separate token\n",
    "    query_with_spaced_dots = query.replace('.', ' . ')\n",
    "    \n",
    "    # Split on whitespace\n",
    "    tokens = query_with_spaced_dots.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_and_create_vocab(input_dir, vocab_size=45000):\n",
    "    word_counts = Counter()\n",
    "\n",
    "    with open(TOKENIZED_FILE_PATH, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        print(\"Starting tokenization and vocabulary creation...\")\n",
    "        \n",
    "        for filename in os.listdir(input_dir):\n",
    "            if not filename.endswith(\".txt\"):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            print(f\"Processing: {filename}\")\n",
    "\n",
    "            for chunk in pd.read_csv(file_path, sep='\\t', names=['userID', 'query'], chunksize=100000):\n",
    "                for query in chunk['query']:\n",
    "                    tokens = tokenize_query(query)\n",
    "                    if tokens:\n",
    "                        word_counts.update(tokens)\n",
    "                        outfile.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "    total_tokens_counted = sum(word_counts.values())\n",
    "    special_tokens = ['<OOV>']\n",
    "    most_common_words = [word for word, _ in word_counts.most_common(vocab_size - len(special_tokens))]\n",
    "\n",
    "    vocabulary = special_tokens + most_common_words\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    actual_vocab_size = len(vocab_dict)\n",
    "    covered_tokens_count = sum(word_counts[word] for word in most_common_words)\n",
    "    coverage_percentage = (covered_tokens_count / total_tokens_counted) * 100 if total_tokens_counted > 0 else 0\n",
    "\n",
    "    vocab_stats = {\n",
    "        \"Requested_Vocabulary_Size\": vocab_size,\n",
    "        \"Actual_Vocabulary_Size\": actual_vocab_size,\n",
    "        \"Total_Tokens_Found\": total_tokens_counted,\n",
    "        \"Total_Unique_Tokens_Found\": len(word_counts),\n",
    "        \"Coverage_Percentage_Of_Top_Tokens\": round(coverage_percentage, 2),\n",
    "        \"Special_Tokens\": special_tokens\n",
    "    }\n",
    "\n",
    "    with open(VOCAB_DICT_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "\n",
    "    with open(VOCAB_STATS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_stats, f)\n",
    "\n",
    "    print(\"✅ Tokenization complete. Saved to:\", TOKENIZED_FILE_PATH)\n",
    "    print(\"✅ Vocabulary saved to:\", VOCAB_DICT_PATH)\n",
    "    print(\"Vocabulary Stats:\")\n",
    "    print(json.dumps(vocab_stats, indent=4))\n",
    "\n",
    "    \n",
    "\n",
    "def get_vocabulary(query_file, vocab_size=45000):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(query_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            word_counter.update(words)\n",
    "\n",
    "    vocab_dict = dict(word_counter.most_common(vocab_size))\n",
    "    \n",
    "    vocab_stats = {\n",
    "        'total_words': sum(word_counter.values()),\n",
    "        'unique_words': len(word_counter),\n",
    "        'vocab_size': len(vocab_dict),\n",
    "    }\n",
    "\n",
    "    return vocab_dict, vocab_stats\n",
    "\n",
    "vocab_size = 45000\n",
    "tokenize_and_create_vocab(INPUT_DIR, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from create_vocabulary import get_vocabulary\n",
    "\n",
    "# n can be changed to evaluate different models, 2-gram, 3-gram etc\n",
    "def load_validation_data(query_file, n=5, sample_size=1000):\n",
    "    val_dataset = []\n",
    "\n",
    "    with open(query_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            for i in range(len(tokens) - n):\n",
    "                input_text = ' '.join(tokens[i:i + n - 1])\n",
    "                true_next_word = tokens[i + n - 1]\n",
    "                val_dataset.append((input_text, true_next_word))\n",
    "\n",
    "    return random.sample(val_dataset, min(sample_size, len(val_dataset)))\n",
    "\n",
    "\n",
    "def calculate_mrr(predictions, actual_word):\n",
    "    for rank, prediction in enumerate(predictions, 1):\n",
    "        if prediction == actual_word:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_rank_of_true_word(model, input_text, true_word, vocabulary):\n",
    "    \n",
    "    # Synca den med MGU-modellen\n",
    "    candidates = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        test_sentence = f\"{input_text} {word}\"\n",
    "        score = model.score(test_sentence)\n",
    "        candidates[word] = score\n",
    "\n",
    "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_k_predictions = [word for word, _ in sorted_candidates[:5]]  # Top-5 predictions\n",
    "    for rank, (word, _) in enumerate(sorted_candidates, 1):\n",
    "        if word == true_word:\n",
    "            return rank, top_k_predictions\n",
    "\n",
    "    return None, top_k_predictions \n",
    "\n",
    "def evaluate_model(model, query_file, vocab_dict, vocab_size=45000, n=5, sample_size=1000):\n",
    "    validation_data = load_validation_data(query_file, n=n, sample_size=sample_size)\n",
    "    vocabulary = list(vocab_dict.keys())\n",
    "\n",
    "    print(f\"\\nEvaluating model on {len(validation_data)} examples...\")\n",
    "\n",
    "    mrr_scores = []\n",
    "    correct_predictions = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (input_text, true_word) in enumerate(validation_data):\n",
    "        rank, top_k_predictions = get_rank_of_true_word(model, input_text, true_word, vocabulary)\n",
    "\n",
    "        mrr = calculate_mrr(top_k_predictions, true_word)\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "        if mrr == 1.0:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        # Showing a few examples\n",
    "        if i < 10:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Input : {input_text}\")\n",
    "            print(f\"True  : {true_word}\")\n",
    "            print(f\"Top-5: {top_k_predictions}\")\n",
    "            print(f\"Rank  : {rank}\")\n",
    "            print(f\"MRR   : {mrr:.4f}\")\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"... Evaluated {i+1}/{len(validation_data)} examples in {elapsed:.2f}s\")\n",
    "\n",
    "    mean_mrr = np.mean(mrr_scores)\n",
    "    accuracy = correct_predictions / len(validation_data)\n",
    "\n",
    "    eval_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Evaluated {len(validation_data)} examples in {eval_time:.2f}s\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mean_mrr:.4f}\")\n",
    "    print(f\"Top-1 Accuracy: {accuracy:.4f} ({correct_predictions}/{len(validation_data)})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Mean_MRR\": mean_mrr,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Total_Examples\": len(validation_data),\n",
    "        \"Correct_Predictions\": correct_predictions\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_path = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/queries_tokenized_5gram.arpa\" \n",
    "    query_file = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/aol_processed/queries_tokenized.txt\"\n",
    "\n",
    "    model = kenlm.Model(model_path)\n",
    "\n",
    "    vocab_dict, vocab_stats = get_vocabulary(query_file = query_file, vocab_size=45000)\n",
    "\n",
    "    results = evaluate_model(\n",
    "        model,\n",
    "        query_file,\n",
    "        vocab_dict,\n",
    "        vocab_size=45000,\n",
    "        n=5, # Do not forget to change to evaluate different models\n",
    "        sample_size=1000\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    print(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Logger and Measuring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import kenlm\n",
    "from pynvml import *\n",
    "from ngram_mrr_evaluation import get_rank_of_true_word\n",
    "from ngram_mrr_evaluation import load_validation_data\n",
    "from create_vocabulary import get_vocabulary\n",
    "\n",
    "# Track resource usage at each step\n",
    "def track_resources():\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    disk_info = psutil.disk_usage('/')\n",
    "    \n",
    "    try:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)  # GPU 0\n",
    "        gpu_util = nvmlDeviceGetUtilizationRates(handle)\n",
    "        gpu_memory = nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_info = {\n",
    "            'gpu_utilization': gpu_util.gpu,\n",
    "            'gpu_memory_used': gpu_memory.used,\n",
    "            'gpu_memory_percent': gpu_memory.used / gpu_memory.total * 100\n",
    "        }\n",
    "        nvmlShutdown()\n",
    "    except:\n",
    "        gpu_info = None\n",
    "    \n",
    "    return {\n",
    "        'cpu': cpu_usage,\n",
    "        'memory_used': memory_info.used,\n",
    "        'memory_percent': memory_info.percent,\n",
    "        'disk_used': disk_info.used,\n",
    "        'disk_percent': disk_info.percent,\n",
    "        'gpu': gpu_info  \n",
    "    }\n",
    "\n",
    "# Function to plot the resource usage as a table\n",
    "def plot_resources_as_table(cpu_data, memory_data, disk_data, gpu_data):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.axis('off')\n",
    "\n",
    "    time_axis = np.arange(len(cpu_data))  # Time axis for rows (number of recorded steps)\n",
    "    \n",
    "    \n",
    "    table_data = [\n",
    "        ['CPU'] + [f'{x:.2f}' for x in cpu_data],\n",
    "        ['Memory'] + [f'{x:.2f}' for x in memory_data['memory_percent']],\n",
    "        ['Disk'] + [f'{x:.2f}' for x in disk_data['disk_percent']],\n",
    "    ]\n",
    "    \n",
    "    if gpu_data:\n",
    "        table_data.append(['GPU'] + [f'{x:.2f}' for x in gpu_data])\n",
    "\n",
    "    table = ax.table(cellText=table_data, colLabels=[f'Time {i+1}' for i in range(len(time_axis))], loc='center', cellLoc='center')\n",
    "    \n",
    "    for i, row in enumerate(table.get_celld().values()):\n",
    "        if i == 0:\n",
    "            row.set_facecolor('lightblue')\n",
    "        elif i == 1:\n",
    "            row.set_facecolor('lightgreen')\n",
    "        elif i == 2:\n",
    "            row.set_facecolor('lightcoral')\n",
    "        elif i == 3 and gpu_data:\n",
    "            row.set_facecolor('lightsalmon')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_kenlm_model(models, query_file, n=5, sample_size=1000):\n",
    "    validation_data = load_validation_data(query_file, n=n, sample_size=sample_size)\n",
    "    \n",
    "    vocab_dict, vocab_stats = get_vocabulary(query_file=query_file, vocab_size=45000)\n",
    "    vocabulary = list(vocab_dict.keys()) \n",
    "\n",
    "    print(f\"Evaluating {len(validation_data)} examples...\")\n",
    "\n",
    "    mrr_scores = {n_gram: [] for n_gram in models.keys()}\n",
    "    accuracy_scores = {n_gram: [] for n_gram in models.keys()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cpu_data = []\n",
    "    memory_data = {'memory_percent': []}\n",
    "    disk_data = {'disk_percent': []}\n",
    "    gpu_data = [] \n",
    "\n",
    "    # Evaluate for each N-gram model (2-gram, 3-gram, 4-gram, 5-gram)\n",
    "    for n_gram, model in models.items():\n",
    "        print(f\"Evaluating {n_gram}-gram model...\")\n",
    "        \n",
    "        for i, (input_text, true_word) in enumerate(validation_data):\n",
    "            # Track resources during evaluation\n",
    "            resources = track_resources()\n",
    "            cpu_data.append(resources['cpu'])\n",
    "            memory_data['memory_percent'].append(resources['memory_percent'])\n",
    "            disk_data['disk_percent'].append(resources['disk_percent'])\n",
    "            if resources['gpu'] is not None:\n",
    "                gpu_data.append(resources['gpu'])\n",
    "\n",
    "            rank, top_k = get_rank_of_true_word(model, input_text, true_word, vocabulary)\n",
    "\n",
    "            if rank is None:\n",
    "                mrr_scores[n_gram].append(0.0)\n",
    "                accuracy_scores[n_gram].append(0)\n",
    "            else:\n",
    "                mrr_scores[n_gram].append(1.0 / rank)\n",
    "                accuracy_scores[n_gram].append(1 if true_word in top_k else 0)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"... Evaluated {i+1}/{len(validation_data)} examples in {elapsed:.2f}s\")\n",
    "\n",
    "    # Calculate average MRR and accuracy for each model\n",
    "    mrr_averages = {n_gram: np.mean(mrr_scores[n_gram]) for n_gram in mrr_scores}\n",
    "    accuracy_averages = {n_gram: np.mean(accuracy_scores[n_gram]) for n_gram in accuracy_scores}\n",
    "\n",
    "    print(\"Mean Reciprocal Rank (MRR):\")\n",
    "    for n_gram in mrr_averages:\n",
    "        print(f\"{n_gram}-gram: {mrr_averages[n_gram]:.4f}\")\n",
    "\n",
    "    print(\"\\nAccuracy:\")\n",
    "    for n_gram in accuracy_averages:\n",
    "        print(f\"{n_gram}-gram: {accuracy_averages[n_gram]:.4f}\")\n",
    "\n",
    "    plot_metrics(mrr_averages, accuracy_averages)\n",
    "    plot_resources_as_table(cpu_data, memory_data, disk_data, gpu_data)\n",
    "\n",
    "# Function to plot MRR and accuracy\n",
    "def plot_metrics(mrr_averages, accuracy_averages):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    n_grams = list(mrr_averages.keys())\n",
    "    mrr_values = list(mrr_averages.values())\n",
    "    accuracy_values = list(accuracy_averages.values())\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(n_grams))\n",
    "\n",
    "    ax.bar(index, mrr_values, bar_width, label='MRR', color='lightblue')\n",
    "    ax.bar(index + bar_width, accuracy_values, bar_width, label='Accuracy', color='lightgreen')\n",
    "\n",
    "    ax.set_xlabel('N-gram Model')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Evaluation of N-gram Models (MRR and Accuracy)')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([f'{n}-gram' for n in n_grams])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "models = {\n",
    "    2: kenlm.Model(\"queries_tokenized_2gram.arpa\"),\n",
    "    3: kenlm.Model(\"queries_tokenized_3gram.arpa\"),\n",
    "    4: kenlm.Model(\"queries_tokenized_4gram.arpa\"),\n",
    "    5: kenlm.Model(\"queries_tokenized_5gram.arpa\"),\n",
    "}\n",
    "\n",
    "query_file = \"C:/Users/enesi/Desktop/DSV/DVK-Uppsats/aol_processed/queries_tokenized.txt\"\n",
    "evaluate_kenlm_model(models, query_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
