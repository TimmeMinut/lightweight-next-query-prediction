{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pCjylYu7X8q",
        "outputId": "92b4b8ab-0d0d-454c-fe54-f42a89398fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title === Imports & Configuration ===\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import psutil\n",
        "import threading\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from keras.src import regularizers\n",
        "from keras.src import constraints\n",
        "\n",
        "# Using a random seed for reproducability\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Mounting notebook to drive for persistant storage of results and models.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = \"/content/drive/MyDrive/MGU\"\n",
        "INPUT_DIR = \"/content/drive/MyDrive/MGU/aol_processed/processed_files\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo | grep 'model name' | head -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJZn-kodripg",
        "outputId": "d7bb7a08-349a-4f70-ad58-3825a25d1acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === MGU Model Implementation ===\n",
        "\n",
        "def build_mgu_model(vocab_size, embedding_dim, mgu_units, max_encoder_length, max_decoder_length):\n",
        "    print(\"\\nBuilding Training Model...\")\n",
        "\n",
        "    # Input layer for encoder\n",
        "    # Since the input has been tokenized to integers we set the datatype to int32\n",
        "    encoder_inputs = keras.Input(shape=(max_encoder_length,), dtype='int32', name='encoder_inputs')\n",
        "\n",
        "    # Embedding layer for encoder\n",
        "    encoder_embedding_layer = layers.Embedding(vocab_size, embedding_dim, mask_zero=True, name='encoder_embedding')\n",
        "    encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "\n",
        "    # MGU layer for encoder\n",
        "    # In teacher forcing, the encoder returns its final state which the decoder starts from along with the target sequence.\n",
        "    encoder_cell = MGUCell(mgu_units, dropout=0.2, recurrent_dropout=0.2)\n",
        "    encoder_mgu_layer = layers.RNN(encoder_cell, return_state=True, name='encoder_mgu')\n",
        "    _, encoder_state = encoder_mgu_layer(encoder_embeddings)\n",
        "\n",
        "    # Input Layer for decoder\n",
        "    decoder_inputs = keras.Input(shape=(max_decoder_length,), dtype='int32', name='decoder_inputs')\n",
        "\n",
        "    # Embedding layer for decoder\n",
        "    decoder_embedding_layer= layers.Embedding(vocab_size, embedding_dim, mask_zero=True, name='decoder_embedding')\n",
        "    decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "    # MGU layer for decoder\n",
        "    decoder_cell = MGUCell(mgu_units, dropout=0.2, recurrent_dropout=0.2)\n",
        "    decoder_mgu_layer = layers.RNN(decoder_cell, return_sequences=True, return_state=True, name='decoder_mgu')\n",
        "    decoder_outputs, _ = decoder_mgu_layer(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "    # Dense layer for output of the decoder\n",
        "    decoder_dense = layers.Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
        "    decoder_predictions = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Takes encoder_inputs and decoder_inputs, and outputs decoder predictions\n",
        "    training_model = keras.Model([encoder_inputs, decoder_inputs], decoder_predictions, name='seq2seq_training_mgu')\n",
        "    training_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    print(\"\\nBuilding Inference Models...\")\n",
        "\n",
        "    # Inference Encoder model\n",
        "    # Reuses the encoder_inputs from the training model\n",
        "    encoder_inf_model = keras.Model(encoder_inputs, encoder_state, name='encoder_inference')\n",
        "\n",
        "    # Inference Decoder model\n",
        "    # Input layer that handles 1 token at a time\n",
        "    decoder_input_single = keras.Input(shape=(1,), dtype='int32', name='decoder_inf_input_single')\n",
        "\n",
        "    # Single step Embedding layer that reuses the same embedding layer from the training model\n",
        "    single_step_decoder_embeddings = decoder_embedding_layer(decoder_input_single)\n",
        "\n",
        "    # Input layer for the decoder state\n",
        "    decoder_state_input = keras.Input(shape=(mgu_units,), name='decoder_inf_state_input')\n",
        "\n",
        "    # Inference decoder MGU layer, reusing the training decoder MGU\n",
        "    decoder_inf_mgu_layer = layers.RNN(decoder_cell, return_state=True, name='decoder_inf_mgu')\n",
        "    decoder_outputs_single, decoder_state_output = decoder_inf_mgu_layer(\n",
        "        single_step_decoder_embeddings, initial_state=decoder_state_input\n",
        "    )\n",
        "\n",
        "    # Reusing the decoder_dense layer from the training model\n",
        "    decoder_predictions_single = decoder_dense(decoder_outputs_single)\n",
        "\n",
        "    # Takes current token + previous state, outputs prediction + new state\n",
        "    decoder_inf_model = keras.Model(\n",
        "        [decoder_input_single, decoder_state_input],\n",
        "        [decoder_predictions_single, decoder_state_output],\n",
        "        name='decoder_inference'\n",
        "    )\n",
        "\n",
        "    return training_model, encoder_inf_model, decoder_inf_model\n",
        "\n",
        "\n",
        "class MGUCell(layers.Layer):\n",
        "\n",
        "    # Minimal Gated Unit (MGU) cell implementation.\n",
        "    # (Adapted from on tensorflow's GRU implementation)\n",
        "\n",
        "    # Implements the equations:\n",
        "    # f_t = σ(W_f[h_{t-1},x_t] + b_f)\n",
        "    # h̃_t = tanh(W_h[f_t⊙h_{t-1},x_t] + b_h)\n",
        "    # h_t = (1-f_t)⊙h_{t-1} + f_t⊙h̃_t\n",
        "\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.0,\n",
        "                 recurrent_dropout=0.0,\n",
        "                 seed=42,\n",
        "                 **kwargs):\n",
        "        if units <= 0:\n",
        "            raise ValueError(\n",
        "                \"Received an invalid value for argument `units`, \"\n",
        "                f\"expected a positive integer, got {units}.\"\n",
        "            )\n",
        "        kwargs.pop(\"implementation\", None)\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = tf.keras.initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1.0, max(0.0, dropout))\n",
        "        self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n",
        "        self.seed = seed\n",
        "\n",
        "        self.state_size = self.units\n",
        "        self.output_size = self.units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        # For more computational efficieny, tensorflows' implementation combines the weight matricies,\n",
        "        # performing one matrix multiplication for the input and one for the previous hidden state.\n",
        "        # The forget gate [W_f] and candidate hidden state [W_h] weights correspond to:\n",
        "        #   their half of the matrix connecting input to hidden state called \"kernel\",\n",
        "        #   their half of matrix connecting previous hidden state to current hidden state: called \"recurrent_kernel\".\n",
        "\n",
        "        # Input weights for the forget gate [W_f] and candidate hidden state [W_h]\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_dim, self.units * 2), # * 2 for [forget, candidate hidden]\n",
        "            name='kernel',\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "\n",
        "        # Hidden state weights for the forget gate [W_f] and candidate hidden state [W_h]\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units * 2), # * 2 for [forget, candidate hidden]\n",
        "            name='recurrent_kernel',\n",
        "            initializer=self.recurrent_initializer,\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint,\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(self.units * 2,), # [forget_bias, candidate_bias]\n",
        "                name='bias',\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "            )\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states, training=False):\n",
        "          h_prev = states[0]  # Previous hidden state\n",
        "\n",
        "          if training and 0.0 < self.dropout < 1.0:\n",
        "              inputs = tf.nn.dropout(inputs, rate=self.dropout, seed=self.seed)\n",
        "\n",
        "          if training and 0.0 < self.recurrent_dropout < 1.0:\n",
        "              h_prev = tf.nn.dropout(h_prev, rate=self.recurrent_dropout, seed=self.seed)\n",
        "\n",
        "\n",
        "          # Compute forget gate and candidate hidden state\n",
        "\n",
        "          # Inputs multiplied by input weights\n",
        "          # This is performing the multiplications:\n",
        "          #    W_f * x_t and W_h * x_t from the equations\n",
        "          # matrix_x = [x_f, x_h] = [forget_gate_input, candidate_input]\n",
        "          matrix_x = tf.matmul(inputs, self.kernel)\n",
        "\n",
        "          # Previous hidden state multipled by candidate hidden state weights\n",
        "          # This is performing the multiplications:\n",
        "          #   W_f * h_{t-1} and W_h * H_{t-1} from the equations\n",
        "          # matrix_h = [h_f, h_h] = [forget_gate_hidden, candidate_hidden]\n",
        "          matrix_h = tf.matmul(h_prev, self.recurrent_kernel)\n",
        "\n",
        "          # bias = [b_f, b_h] applied before split\n",
        "          if self.use_bias:\n",
        "              matrix_x = tf.nn.bias_add(matrix_x, self.bias)\n",
        "\n",
        "\n",
        "          # Split the combined computations into forget and candidate hidden parts\n",
        "          # Split input projections: [x_f, x_h] = [forget_gate_input, candidate_input]\n",
        "          x_f, x_h = tf.split(matrix_x, 2, axis=-1)\n",
        "\n",
        "          # Split hidden state projections: [h_f, h_h] = [forget_gate_hidden, candidate_hidden]\n",
        "          h_f, h_h = tf.split(matrix_h, 2, axis=-1)  # We only need h_f here\n",
        "\n",
        "          # Compute forget gate: f_t = σ(W_f[h_{t-1},x_t] + b_f)\n",
        "          # Which can be expanded for clarification to: f_t = σ(W_f·x_t + W_f·h_{t-1} + b_f)\n",
        "          # (The biases were added before splitting, so we dont need to add them here)\n",
        "          # recuccent_activation = σ\n",
        "          f = self.recurrent_activation(x_f + h_f)\n",
        "\n",
        "          # For the candidate hidden state state: h̃_t = tanh(W_h[f_t⊙h_{t-1},x_t] + b_h)\n",
        "          # Compute element-wise multiplication of the forget gate and previous hidden state\n",
        "          #    f_t ⊙ h_{t-1}\n",
        "          f_h_prev = f * h_prev\n",
        "\n",
        "          # Now multiply (f_t⊙h_{t-1}) with W_h\n",
        "          #   We've already computed the x_t part (x_h)\n",
        "          # Note: we reuse the second half of the recurrennt_kernel which holds\n",
        "          #   the weight matrix between prev hidden state and candidate hidden state\n",
        "          f_h_prev_h = tf.matmul(\n",
        "              f_h_prev, self.recurrent_kernel[:, self.units:])\n",
        "\n",
        "          # Compute candidate state:  h̃_t = tanh(W_h[f_t⊙h_{t-1},x_t] + b_h)\n",
        "          h_tilde = self.activation(x_h + f_h_prev_h)\n",
        "\n",
        "          # Compute new hidden state: h_t = (1-f_t)⊙h_{t-1} + f_t⊙h̃_t\n",
        "          h = (1 - f) * h_prev + f * h_tilde\n",
        "\n",
        "          # Returns the output and a list of states to be passed to the next timestep.\n",
        "          return h, [h]\n",
        "\n",
        "    def get_config(self):\n",
        "            config = {\n",
        "                \"units\": self.units,\n",
        "                \"activation\": tf.keras.activations.serialize(self.activation),\n",
        "                \"recurrent_activation\": tf.keras.activations.serialize(\n",
        "                    self.recurrent_activation\n",
        "                ),\n",
        "                \"use_bias\": self.use_bias,\n",
        "                \"kernel_initializer\": tf.keras.initializers.serialize(\n",
        "                    self.kernel_initializer\n",
        "                ),\n",
        "                \"recurrent_initializer\": tf.keras.initializers.serialize(\n",
        "                    self.recurrent_initializer\n",
        "                ),\n",
        "                \"bias_initializer\": tf.keras.initializers.serialize(self.bias_initializer),\n",
        "                \"dropout\": self.dropout,\n",
        "                \"recurrent_dropout\": self.recurrent_dropout\n",
        "            }\n",
        "            base_config = super().get_config()\n",
        "            return {**base_config, **config}\n",
        "\n",
        "@classmethod\n",
        "def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "# Register so the custom cell can properly be saved and loaded by keras\n",
        "tf.keras.utils.get_custom_objects().update({'MGUCell': MGUCell})"
      ],
      "metadata": {
        "id": "MUnot7wp8Om6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === Data Processing Functions ===\n",
        "\n",
        "def create_vocabulary(input_dir, vocab_size=45000):\n",
        "            word_counts = Counter()\n",
        "\n",
        "            print(\"Building vocabulary using modified tokenization (split on '.' and whitespace)...\")\n",
        "            for filename in os.listdir(input_dir):\n",
        "                if not filename.endswith('.txt'):\n",
        "                    continue\n",
        "\n",
        "                file_path = os.path.join(input_dir, filename)\n",
        "                print(f\"Counting tokens from: {filename}\")\n",
        "\n",
        "                for chunk in pd.read_csv(file_path, sep='\\t', names=['userID', 'query'], chunksize=100000):\n",
        "                    for query in chunk['query'].astype(str):\n",
        "                        query_with_spaced_dots = query.replace('.', ' . ')\n",
        "                        tokens = query_with_spaced_dots.split()\n",
        "                        word_counts.update(tokens)\n",
        "\n",
        "            total_tokens_counted = sum(word_counts.values())\n",
        "\n",
        "            special_tokens = ['<PAD>', '<OOV>', '<START>', '<SEP>', '<END>']\n",
        "\n",
        "            most_common_words = [\n",
        "                word for word, _ in word_counts.most_common(vocab_size - len(special_tokens))\n",
        "            ]\n",
        "\n",
        "            vocabulary = special_tokens + most_common_words\n",
        "            vocab_dict = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "            actual_vocab_size = len(vocab_dict)\n",
        "\n",
        "\n",
        "            total_tokens_counted = sum(word_counts.values())\n",
        "            covered_tokens_count = sum(word_counts[word] for word in most_common_words)\n",
        "            coverage_percentage = (covered_tokens_count / total_tokens_counted) * 100 if total_tokens_counted > 0 else 0\n",
        "\n",
        "            vocab_stats = {\n",
        "                \"Requested_Vocabulary_Size\": vocab_size,\n",
        "                \"Actual_Vocabulary_Size\": actual_vocab_size,\n",
        "                \"Total_Tokens_Found\": total_tokens_counted,\n",
        "                \"Total_Unique_Tokens_Found\": len(word_counts),\n",
        "                \"Coverage_Percentage_Of_Top_Tokens\": round(coverage_percentage, 2),\n",
        "                \"Special_Tokens\": special_tokens\n",
        "            }\n",
        "            print(\"Vocabulary Stats:\")\n",
        "            print(json.dumps(vocab_stats, indent=4))\n",
        "\n",
        "\n",
        "            return vocab_dict, vocab_stats\n",
        "\n",
        "def get_vocabulary(vocab_size):\n",
        "    vocab_path = f\"{BASE_PATH}/vocab_dict.json\"\n",
        "    vocab_stats_path = f\"{BASE_PATH}/vocab_stats.json\"\n",
        "\n",
        "    # Load existing vocabulary\n",
        "    if os.path.exists(vocab_path) and os.path.exists(vocab_stats_path):\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            vocab_dict = json.load(f)\n",
        "            if len(vocab_dict) == vocab_size:\n",
        "              with open(vocab_stats_path, 'r') as f:\n",
        "                vocab_stats = json.load(f)\n",
        "              print(\"Loading existing vocabulary\")\n",
        "              return vocab_dict, vocab_stats\n",
        "\n",
        "    # Or generate new one\n",
        "    print(\"Creating new vocabulary\")\n",
        "    vocab_dict, vocab_stats = create_vocabulary(INPUT_DIR, vocab_size)\n",
        "    with open(vocab_path, 'w') as f1, open(vocab_stats_path, 'w') as f2:\n",
        "      json.dump(vocab_dict, f1)\n",
        "      json.dump(vocab_stats, f2)\n",
        "\n",
        "    print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
        "    return vocab_dict, vocab_stats\n",
        "\n",
        "\n",
        "def tokenize_text(text, vocab_dict):\n",
        "    oov = vocab_dict['<OOV>']\n",
        "    text = str(text)\n",
        "\n",
        "    text_with_spaced_dots = text.replace('.', ' . ')\n",
        "    words = text_with_spaced_dots.split()\n",
        "\n",
        "    token_ids = [vocab_dict.get(word, oov) for word in words]\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def prepare_training_data(input_dir, vocab_dict, context_length, test_split, max_encoder_length, max_decoder_length, batch_size):\n",
        "\n",
        "    start_token = vocab_dict['<START>']\n",
        "    pad_token = vocab_dict['<PAD>']\n",
        "    sep_token = vocab_dict['<SEP>']\n",
        "    end_token = vocab_dict['<END>']\n",
        "\n",
        "    print(f\"Preparing training data...\")\n",
        "\n",
        "    encoder_inputs = []\n",
        "    decoder_inputs = []\n",
        "    decoder_targets = []\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if not filename.endswith('.txt'):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "        print(f\"Processing {filename}...\")\n",
        "\n",
        "        for chunk in pd.read_csv(file_path, sep='\\t', names=['userID', 'query'], chunksize=50000):\n",
        "            chunk['query'] = chunk['query'].astype(str)\n",
        "            user_queries = chunk.groupby('userID')['query'].apply(list).reset_index()\n",
        "\n",
        "            for _, user in user_queries.iterrows():\n",
        "                queries = user['query']\n",
        "\n",
        "                if len(queries) < context_length + 1:\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(queries) - context_length):\n",
        "                    # Get context and target\n",
        "                    context_queries = queries[i:i+context_length]\n",
        "                    target_query = queries[i+context_length]\n",
        "\n",
        "                    context_tokens_list = []\n",
        "                    for query in context_queries:\n",
        "                        query_tokens = tokenize_text(query, vocab_dict)\n",
        "                        if context_tokens_list:\n",
        "                            context_tokens_list.append(sep_token)\n",
        "                        context_tokens_list.extend(query_tokens)\n",
        "\n",
        "                    target_tokens = tokenize_text(target_query, vocab_dict)\n",
        "                    target_tokens = target_tokens + [end_token]\n",
        "\n",
        "                    if not target_tokens:\n",
        "                        continue\n",
        "\n",
        "                    decoder_input = [start_token] + target_tokens[:-1]\n",
        "\n",
        "                    decoder_target = target_tokens\n",
        "\n",
        "                    if context_tokens_list and decoder_input and decoder_target:\n",
        "                        encoder_inputs.append(context_tokens_list)\n",
        "                        decoder_inputs.append(decoder_input)\n",
        "                        decoder_targets.append(decoder_target)\n",
        "\n",
        "\n",
        "    print(f\"\\nFinished processing data. Found {len(encoder_inputs)} valid sequences.\")\n",
        "    if not encoder_inputs:\n",
        "        raise ValueError(\"No valid sequences found after processing data. Check input data and tokenization.\")\n",
        "\n",
        "    print(f\"Truncating sequences to max_encoder_length={max_encoder_length}, max_decoder_length={max_decoder_length}\")\n",
        "    encoder_inputs = [seq[:max_encoder_length] for seq in encoder_inputs]\n",
        "    decoder_inputs = [seq[:max_decoder_length] for seq in decoder_inputs]\n",
        "    decoder_targets = [seq[:max_decoder_length] for seq in decoder_targets]\n",
        "\n",
        "\n",
        "    print(\"Padding sequences...\")\n",
        "    encoder_inputs_padded = keras.preprocessing.sequence.pad_sequences(\n",
        "          encoder_inputs, maxlen=max_encoder_length, padding='post', value=pad_token)\n",
        "    decoder_inputs_padded = keras.preprocessing.sequence.pad_sequences(\n",
        "          decoder_inputs, maxlen=max_decoder_length, padding='post', value=pad_token)\n",
        "    decoder_targets_padded = keras.preprocessing.sequence.pad_sequences(\n",
        "          decoder_targets, maxlen=max_decoder_length, padding='post', value=pad_token)\n",
        "\n",
        "    print(\"Splitting data into train/validation sets...\")\n",
        "    (encoder_train, encoder_val,\n",
        "     decoder_input_train, decoder_input_val,\n",
        "     decoder_target_train, decoder_target_val) = sklearn.model_selection.train_test_split(\n",
        "        encoder_inputs_padded, decoder_inputs_padded, decoder_targets_padded,\n",
        "        test_size=test_split, random_state=42\n",
        "    )\n",
        "    print(\"Data splitting complete.\")\n",
        "\n",
        "    print(\"Creating tf.data Datasets...\")\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ((encoder_train, decoder_input_train), decoder_target_train)\n",
        "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ((encoder_val, decoder_input_val), decoder_target_val)\n",
        "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    print(\"Dataset creation complete.\")\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def load_or_create_datasets(input_dir, vocab_dict, context_length, test_split, max_encoder_length, max_decoder_length, dataset_dir, batch_size, force_regenerate=False,):\n",
        "    train_save_path = os.path.join(dataset_dir, f\"training_set_CL{context_length}\")\n",
        "    val_save_path = os.path.join(dataset_dir, f\"validation_set_CL{context_length}\")\n",
        "\n",
        "    if (os.path.exists(train_save_path) and\n",
        "        os.path.exists(val_save_path) and\n",
        "        not force_regenerate):\n",
        "\n",
        "        print(f\"Loading datasets from {dataset_dir}...\")\n",
        "        start_load = time.time()\n",
        "        try:\n",
        "            train_dataset = tf.data.Dataset.load(train_save_path)\n",
        "            val_dataset = tf.data.Dataset.load(val_save_path)\n",
        "            print(f\"Datasets loaded successfully in {time.time() - start_load:.2f} seconds.\")\n",
        "            return train_dataset, val_dataset\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading datasets from {dataset_dir}: {e}\")\n",
        "            print(\"Will attempt to regenerate the datasets.\")\n",
        "\n",
        "    print(\"Generating new datasets...\")\n",
        "    start_generate = time.time()\n",
        "\n",
        "    train_dataset, val_dataset = prepare_training_data(\n",
        "        input_dir=input_dir,\n",
        "        vocab_dict=vocab_dict,\n",
        "        context_length=context_length,\n",
        "        test_split=test_split,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        max_decoder_length=max_decoder_length,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset generation finished in {time.time() - start_generate:.2f} seconds.\")\n",
        "\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving datasets to {dataset_dir}...\")\n",
        "    start_save = time.time()\n",
        "    try:\n",
        "        train_dataset.save(train_save_path)\n",
        "        val_dataset.save(val_save_path)\n",
        "        print(f\"Datasets saved successfully in {time.time() - start_save:.2f} seconds.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving datasets to {dataset_dir}: {e}\")\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "XQEVpUPR8jk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === Prediction & Evaluation Functions ===\n",
        "\n",
        "def calculate_mrr(predictions, actual_tokens):\n",
        "    for rank, prediction in enumerate(predictions, 1):\n",
        "        if prediction == actual_tokens:\n",
        "            return 1.0 / rank\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "def reconstruct_text(token_ids, index_to_word):\n",
        "    words = [index_to_word.get(token_id, '<OOV>') for token_id in token_ids]\n",
        "\n",
        "    if not words:\n",
        "        return \"\"\n",
        "\n",
        "    reconstructed = \"\"\n",
        "    for i, word in enumerate(words):\n",
        "        # Add space before the current word unless it's the first word,\n",
        "        # or the previous word was '.', or the current word is '.'\n",
        "        if i > 0 and words[i-1] != '.' and word != '.':\n",
        "            reconstructed += \" \"\n",
        "        reconstructed += word\n",
        "    return reconstructed\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_predictions(inf_encoder_model, inf_decoder_model, input_seq, vocab_dict, top_k=5, max_length=20):\n",
        "    # Generates k predictions by:\n",
        "    #   Predicting the k most likely first tokens after <START>.\n",
        "    #   Running greedy decoding for each of those k starting tokens.\n",
        "\n",
        "    start_token = vocab_dict['<START>']\n",
        "    end_token = vocab_dict['<END>']\n",
        "    pad_token = vocab_dict['<PAD>']\n",
        "    predictions = []\n",
        "\n",
        "    # Prepare initial input (<START> token) and state for the decoder\n",
        "    initial_decoder_input = tf.constant([[start_token]], dtype=tf.int32)\n",
        "    initial_decoder_state = inf_encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Predict the first step\n",
        "    first_step_predictions, first_step_state = inf_decoder_model.predict(\n",
        "        [initial_decoder_input, initial_decoder_state],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Get the top-k first tokens\n",
        "    first_token_probabilities = first_step_predictions[0]\n",
        "    top_k_values, top_k_indices = tf.math.top_k(first_token_probabilities, k=top_k)\n",
        "    top_k_first_token_ids = top_k_indices.numpy()\n",
        "\n",
        "    # Generate a full query prediction for each top-k starting token\n",
        "    for first_token_id in top_k_first_token_ids:\n",
        "        generated_token_ids = []\n",
        "\n",
        "        if first_token_id == end_token or first_token_id == pad_token:\n",
        "            predictions.append(\"\")\n",
        "            continue\n",
        "\n",
        "        generated_token_ids.append(first_token_id)\n",
        "        # Prepare initial input and state following the first token\n",
        "        current_input = tf.constant([[first_token_id]], dtype=tf.int32)\n",
        "        current_states_copy = tf.identity(first_step_state)\n",
        "\n",
        "        # Recurrently generate and pass along token predictions and cell states\n",
        "        # Until either max_length is reached or a <END> or <PAD> token is predicted\n",
        "        for _ in range(max_length - 1):\n",
        "            output_tokens, new_decoder_state = inf_decoder_model.predict(\n",
        "                [current_input, current_states_copy],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            next_token = np.argmax(output_tokens[0])\n",
        "\n",
        "            if next_token == end_token or next_token == pad_token:\n",
        "                break\n",
        "\n",
        "            generated_token_ids.append(next_token)\n",
        "\n",
        "            current_input = tf.constant([[next_token]], dtype=tf.int32)\n",
        "            current_states_copy = new_decoder_state\n",
        "\n",
        "            if len(generated_token_ids) >= max_length:\n",
        "                break\n",
        "\n",
        "        predictions.append(generated_token_ids)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def batch_get_predictions(inf_encoder_model, inf_decoder_model, val_dataset, vocab_dict, top_k=5, max_examples=1000, max_length=20):\n",
        "    results = []\n",
        "    count = 0\n",
        "    pad_token_id = vocab_dict['<PAD>']\n",
        "    start_token_id = vocab_dict['<START>']\n",
        "    end_token_id = vocab_dict['<END>']\n",
        "\n",
        "    generation_start_time = time.time()\n",
        "\n",
        "    for inputs_batch, targets_batch in val_dataset:\n",
        "        if max_examples is not None and count >= max_examples:\n",
        "            break\n",
        "\n",
        "        encoder_inputs, _ = inputs_batch\n",
        "        batch_size = tf.shape(encoder_inputs)[0].numpy()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if max_examples is not None and count >= max_examples:\n",
        "                break\n",
        "\n",
        "            encoder_input = encoder_inputs[i:i+1]  # Keep batch dimension\n",
        "\n",
        "            input_ids = encoder_input[0].numpy()\n",
        "            input_ids = [iid for iid in input_ids if iid != pad_token_id]\n",
        "\n",
        "            target_ids = targets_batch[i].numpy()\n",
        "            target_ids = [tid for tid in target_ids if tid != pad_token_id and tid != end_token_id]\n",
        "\n",
        "            predictions = get_top_k_predictions(\n",
        "                inf_encoder_model,\n",
        "                inf_decoder_model,\n",
        "                encoder_input,\n",
        "                vocab_dict,\n",
        "                top_k=top_k,\n",
        "                max_length=max_length\n",
        "            )\n",
        "\n",
        "            if (count + 1) % 100 == 0:\n",
        "                 current_time = time.time()\n",
        "                 print(f\"... Generated predictions for {count + 1} examples (Total time: {current_time - generation_start_time:.2f}s) ...\")\n",
        "\n",
        "            result = {\n",
        "                \"input\": input_ids,\n",
        "                \"actual\": target_ids,\n",
        "                \"predictions\": predictions\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "            count += 1\n",
        "\n",
        "    total_gen_time = time.time() - generation_start_time\n",
        "    print(f\"\\nFinished generating predictions for {count} examples in {total_gen_time:.2f} seconds.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_model(inf_encoder_model, inf_decoder_model, val_dataset, index_to_word, vocab_dict, context_length, top_k=5, max_examples=1000, max_length=30):\n",
        "\n",
        "    print(f\"\\nEvaluating model using inference models for context length {context_length}...\")\n",
        "    overall_start_time = time.time()\n",
        "\n",
        "    print(\"\\n Getting predictions in batches...\")\n",
        "    results = batch_get_predictions(\n",
        "        inf_encoder_model,\n",
        "        inf_decoder_model,\n",
        "        val_dataset,\n",
        "        vocab_dict,\n",
        "        top_k=top_k,\n",
        "        max_examples=max_examples,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    print(\"\\nCalculating MRR metrics...\")\n",
        "    metrics_start_time = time.time()\n",
        "\n",
        "    mrr_scores = []\n",
        "    correct_predictions = 0\n",
        "    total_examples_processed = len(results)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        input_ids = result[\"input\"]\n",
        "        actual_ids = result[\"actual\"]\n",
        "        predictions = result[\"predictions\"]\n",
        "\n",
        "        mrr = calculate_mrr(predictions, actual_ids)\n",
        "        mrr_scores.append(mrr)\n",
        "\n",
        "        # Check top-1 accuracy\n",
        "        if mrr == 1.0:\n",
        "             correct_predictions += 1\n",
        "\n",
        "        if i < 10:\n",
        "            input_text = reconstruct_text(input_ids, index_to_word)\n",
        "            actual_text = reconstruct_text(actual_ids, index_to_word)\n",
        "            predictions_text = [reconstruct_text(pred, index_to_word) for pred in predictions]\n",
        "\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Context Length : {context_length}\")\n",
        "            print(f\"Input : {input_text}\")\n",
        "            print(f\"Actual: {actual_text}\")\n",
        "            print(f\"Preds :\")\n",
        "            for rank, pred in enumerate(predictions_text, 1):\n",
        "              print(f\"\\t{rank}. {pred} {'<<< MATCH!' if pred == actual_text else ''}\")\n",
        "            print(f\"MRR   : {mrr:.4f}\")\n",
        "        elif (i + 1) % 100 == 0:\n",
        "             current_time = time.time()\n",
        "             print(f\"... Calculated MRR for {i + 1}/{total_examples_processed} examples (Time: {current_time - metrics_start_time:.2f}s) ...\")\n",
        "\n",
        "    # Calculate overall / global MRR for the whole model\n",
        "    mean_mrr = np.mean(mrr_scores)\n",
        "    accuracy = correct_predictions / total_examples_processed\n",
        "    eval_time = time.time() - overall_start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Evaluation Results:\")\n",
        "    print(f\"Evaluated {total_examples_processed} examples in {eval_time:.2f} seconds\")\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mean_mrr:.4f}\")\n",
        "    print(f\"Top-1 Accuracy: {accuracy:.4f} ({correct_predictions}/{total_examples_processed})\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return {\n",
        "        \"Mean_MRR\": float(mean_mrr),\n",
        "        \"Accuracy\": float(accuracy), # Top-1 accuracy\n",
        "        \"Total_Examples\": total_examples_processed,\n",
        "        \"Correct_Predictions\": correct_predictions\n",
        "    }"
      ],
      "metadata": {
        "id": "XllwGDQ98tm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === Plotting ===\n",
        "\n",
        "def plot_history(history, context_length):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Accuracy subplot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "\n",
        "    # Loss subplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{BASE_PATH}/results/training_history_plots_CL{context_length}.pdf\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "4jrq9hr78xOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === System Resource Logging ===\n",
        "\n",
        "class ResourceLogger:\n",
        "    def __init__(self, output_path, interval=300):\n",
        "        self.output_path = os.path.abspath(output_path)\n",
        "        self.output_dir = os.path.dirname(self.output_path)\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.interval = interval\n",
        "        self.stop_event = threading.Event()\n",
        "        self.logs = []\n",
        "        self.thread = None\n",
        "        self.start_time = None\n",
        "\n",
        "        with open(self.output_path, 'w') as f:\n",
        "            f.write('[]')\n",
        "\n",
        "    def _collect_resources(self):\n",
        "        elapsed_seconds = time.time() - self.start_time\n",
        "\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "        memory = psutil.virtual_memory()\n",
        "        disk = psutil.disk_usage('/')\n",
        "\n",
        "        gpu_resources = []\n",
        "        command = [\n",
        "            \"nvidia-smi\",\n",
        "            \"--query-gpu=index,name,utilization.gpu,memory.total,memory.used,memory.free\",\n",
        "            \"--format=csv,noheader,nounits\"\n",
        "        ]\n",
        "\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            encoding='utf-8'\n",
        "        )\n",
        "\n",
        "        gpu_output = result.stdout.strip()\n",
        "\n",
        "        if gpu_output:\n",
        "            lines = gpu_output.split('\\n')\n",
        "            for line in lines:\n",
        "                if not line: continue\n",
        "\n",
        "\n",
        "                idx_str, name, util_str, mem_total_str, mem_used_str, mem_free_str = line.split(',')\n",
        "\n",
        "                gpu_resources.append({\n",
        "                    'gpu_id': int(idx_str.strip()),\n",
        "                    'gpu_name': name.strip(),\n",
        "                    'gpu_load': float(util_str.strip()), # Utilization %\n",
        "                    'gpu_memory_total': int(mem_total_str.strip()),\n",
        "                    'gpu_memory_used': int(mem_used_str.strip()),\n",
        "                    'gpu_memory_free': int(mem_free_str.strip())\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'elapsed_seconds': elapsed_seconds,\n",
        "            'cpu_percent': cpu_percent,\n",
        "            'memory': {\n",
        "                'total': memory.total,\n",
        "                'available': memory.available,\n",
        "                'used': memory.used,\n",
        "                'percent': memory.percent\n",
        "            },\n",
        "            'disk': {\n",
        "                'total': disk.total,\n",
        "                'used': disk.used,\n",
        "                'free': disk.free,\n",
        "                'percent': disk.percent\n",
        "            },\n",
        "            'gpus': gpu_resources\n",
        "        }\n",
        "\n",
        "    def _logging_thread(self):\n",
        "        while not self.stop_event.is_set():\n",
        "            next_log_time = time.time() + self.interval\n",
        "            try:\n",
        "                resource_entry = self._collect_resources()\n",
        "                self.logs.append(resource_entry)\n",
        "                print(f\"Logged resource entry at elapsed time: {resource_entry['elapsed_seconds']:.2f}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during resource collection in logging thread: {e}\")\n",
        "\n",
        "            wait_time = max(0, next_log_time - time.time())\n",
        "            self.stop_event.wait(timeout=wait_time)\n",
        "\n",
        "    def start(self):\n",
        "        if self.thread is not None and self.thread.is_alive():\n",
        "            print(\"Logger thread already running.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Starting resource logger. Interval: {self.interval}s. Output: {self.output_path}\")\n",
        "        self.stop_event.clear()\n",
        "        self.logs = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        self.thread = threading.Thread(target=self._logging_thread, daemon=True)\n",
        "        self.thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.thread is None or not self.thread.is_alive():\n",
        "            print(\"Logger thread not running or already stopped.\")\n",
        "            return\n",
        "\n",
        "        print(\"Stopping resource logger...\")\n",
        "        self.stop_event.set()\n",
        "\n",
        "        self.thread.join(timeout=5.0)\n",
        "\n",
        "        if self.thread.is_alive():\n",
        "            print(\"Warning: Logger thread did not stop gracefully.\")\n",
        "\n",
        "        self._save_logs()\n",
        "        self.thread = None\n",
        "\n",
        "    def _save_logs(self):\n",
        "        print(f\"Saving {len(self.logs)} resource log entries...\")\n",
        "\n",
        "        with open(self.output_path, 'w') as f:\n",
        "            json.dump(self.logs, f, indent=2)\n",
        "        print(f\"Resource logs saved successfully to {self.output_path}\")\n",
        "        print(f\"Number of log entries: {len(self.logs)}\")"
      ],
      "metadata": {
        "id": "H8KCi7P8am2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title === Training And Evaluation of Models ===\n",
        "\n",
        "# Experiment settings\n",
        "CONTEXT_LENGTHS = [1, 2, 3, 4, 5]\n",
        "MAX_EXAMPLES = 1000\n",
        "TOP_K = 5\n",
        "\n",
        "# Model hyperparameters\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 10\n",
        "MAX_DECODER_LENGTH = 30\n",
        "VOCABULARY_SIZE = 45000\n",
        "TEST_SPLIT = 0.2\n",
        "\n",
        "def train_models():\n",
        "    os.makedirs(f\"{BASE_PATH}/models\", exist_ok=True)\n",
        "    os.makedirs(f\"{BASE_PATH}/results\", exist_ok=True)\n",
        "    os.makedirs(f\"{BASE_PATH}/data_cache\", exist_ok=True)\n",
        "\n",
        "    print(f\"Training with: Batch={BATCH_SIZE}, Embedding={EMBEDDING_DIM}, Hidden={HIDDEN_DIM}, Epochs={EPOCHS}\")\n",
        "    print(f\"Context lengths to evaluate: {CONTEXT_LENGTHS}\")\n",
        "\n",
        "    vocab_dict, vocab_stats = get_vocabulary(VOCABULARY_SIZE)\n",
        "\n",
        "    print(\"Vocabulary Statistics:\")\n",
        "    print(json.dumps(vocab_stats, indent=4))\n",
        "\n",
        "    for context_length in CONTEXT_LENGTHS:\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Current context length: {context_length}\")\n",
        "\n",
        "        MAX_ENCODER_LENGTH = MAX_DECODER_LENGTH * context_length\n",
        "\n",
        "        resource_log_path = f\"{BASE_PATH}/results/system_resources_training_CL{context_length}.json\"\n",
        "        resource_logger = ResourceLogger(resource_log_path, interval=150)  # Log every 2.5 minutes\n",
        "\n",
        "        training_model_path = f\"{BASE_PATH}/models/MGU_Training_CL{context_length}.keras\"\n",
        "        inf_encoder_model_path = f\"{BASE_PATH}/models/MGU_Inference_Encoder_CL{context_length}.keras\"\n",
        "        inf_decoder_model_path = f\"{BASE_PATH}/models/MGU_Inference_Decoder_CL{context_length}.keras\"\n",
        "\n",
        "        try:\n",
        "            resource_logger.start()\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_dataset, val_dataset = load_or_create_datasets(\n",
        "                input_dir=INPUT_DIR,\n",
        "                vocab_dict=vocab_dict,\n",
        "                context_length=context_length,\n",
        "                test_split=TEST_SPLIT,\n",
        "                max_encoder_length=MAX_ENCODER_LENGTH,\n",
        "                max_decoder_length=MAX_DECODER_LENGTH,\n",
        "                dataset_dir=f\"{BASE_PATH}/data_cache\",\n",
        "                batch_size=BATCH_SIZE,\n",
        "                force_regenerate=True\n",
        "            )\n",
        "\n",
        "            data_prep_time = time.time() - start_time\n",
        "            print(f\"Preparation of dataset completed in {data_prep_time:.2f} seconds\")\n",
        "\n",
        "            if os.path.exists(training_model_path) and os.path.exists(inf_encoder_model_path) and os.path.exists(inf_decoder_model_path):\n",
        "                print(f\"Loading existing models:\")\n",
        "                print(f\"  - Training model: {training_model_path}\")\n",
        "                print(f\"  - Inference encoder: {inf_encoder_model_path}\")\n",
        "                print(f\"  - Inference decoder: {inf_decoder_model_path}\")\n",
        "\n",
        "                training_model = tf.keras.models.load_model(\n",
        "                    training_model_path,\n",
        "                    custom_objects={'MGUCell': MGUCell}\n",
        "                )\n",
        "                inf_encoder_model = tf.keras.models.load_model(\n",
        "                    inf_encoder_model_path,\n",
        "                    custom_objects={'MGUCell': MGUCell}\n",
        "                )\n",
        "                inf_decoder_model = tf.keras.models.load_model(\n",
        "                    inf_decoder_model_path,\n",
        "                    custom_objects={'MGUCell': MGUCell}\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                print(f\"No existing models found. Building and training new models...\")\n",
        "\n",
        "                training_model, inf_encoder_model, inf_decoder_model = build_mgu_model(\n",
        "                    VOCABULARY_SIZE, EMBEDDING_DIM, HIDDEN_DIM, MAX_ENCODER_LENGTH, MAX_DECODER_LENGTH)\n",
        "\n",
        "                training_model.summary()\n",
        "                inf_encoder_model.summary()\n",
        "                inf_decoder_model.summary()\n",
        "\n",
        "                keras.utils.plot_model(training_model,\n",
        "                      show_shapes=True,\n",
        "                      show_layer_names=True,\n",
        "                      show_layer_activations=True,\n",
        "                      to_file=f'{BASE_PATH}/results/training_model_plot_CL{context_length}.pdf'\n",
        "                )\n",
        "                keras.utils.plot_model(inf_encoder_model,\n",
        "                      show_shapes=True,\n",
        "                      show_layer_names=True,\n",
        "                      show_layer_activations=True,\n",
        "                      to_file=f'{BASE_PATH}/results/inf_encoder_model_plot_CL{context_length}.pdf'\n",
        "                )\n",
        "                keras.utils.plot_model(inf_decoder_model,\n",
        "                      show_shapes=True,\n",
        "                      show_layer_names=True,\n",
        "                      show_layer_activations=True,\n",
        "                      to_file=f'{BASE_PATH}/results/inf_decoder_model_plot_CL{context_length}.pdf'\n",
        "                )\n",
        "\n",
        "                with open(f'{BASE_PATH}/results/training_model_summary_CL{context_length}.txt', 'w') as f:\n",
        "                  training_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "                with open(f'{BASE_PATH}/results/inf_encoder_model_summary_CL{context_length}.txt', 'w') as f:\n",
        "                  inf_encoder_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "                with open(f'{BASE_PATH}/results/inf_decoder_model_summary_CL{context_length}.txt', 'w') as f:\n",
        "                  inf_decoder_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "                callbacks = [\n",
        "                    tf.keras.callbacks.ModelCheckpoint(\n",
        "                        training_model_path,\n",
        "                        save_best_only=True,\n",
        "                        monitor='val_loss',\n",
        "                        mode='min'\n",
        "                    ),\n",
        "                    tf.keras.callbacks.EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        mode='min',\n",
        "                        patience=3,\n",
        "                        verbose=1,\n",
        "                        restore_best_weights=True\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "                print(f\"Training model...\")\n",
        "                start_time = time.time()\n",
        "                history = training_model.fit(\n",
        "                    train_dataset,\n",
        "                    validation_data=val_dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks\n",
        "                )\n",
        "                training_time = time.time() - start_time\n",
        "                print(f\"Model training finished in {training_time:.2f} seconds\")\n",
        "\n",
        "                with open(f'{BASE_PATH}/results/training_history_CL{context_length}.json', 'w') as f:\n",
        "                  json.dump(history.history, f, indent=4)\n",
        "\n",
        "                training_model.save(training_model_path)\n",
        "                inf_encoder_model.save(inf_encoder_model_path)\n",
        "                inf_decoder_model.save(inf_decoder_model_path)\n",
        "\n",
        "                plot_history(history, context_length)\n",
        "\n",
        "        finally:\n",
        "            resource_logger.stop()\n",
        "            print(\"End of train models block reached\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_models():\n",
        "    os.makedirs(f\"{BASE_PATH}/results\", exist_ok=True)\n",
        "\n",
        "    print(f\"Context lengths to evaluate: {CONTEXT_LENGTHS}\")\n",
        "\n",
        "    vocab_dict, vocab_stats = get_vocabulary(VOCABULARY_SIZE)\n",
        "    index_to_word = {idx: word for word, idx in vocab_dict.items()}\n",
        "\n",
        "    print(\"Vocabulary Statistics:\")\n",
        "    print(json.dumps(vocab_stats, indent=4))\n",
        "\n",
        "    for context_length in CONTEXT_LENGTHS:\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Evaluating models with context length: {context_length}\")\n",
        "\n",
        "        MAX_ENCODER_LENGTH = MAX_DECODER_LENGTH * context_length\n",
        "\n",
        "        resource_log_path = f\"{BASE_PATH}/results/system_resources_inference_CL{context_length}.json\"\n",
        "        resource_logger = ResourceLogger(resource_log_path, interval=150)  # Log every 2.5 minutes\n",
        "\n",
        "        inf_encoder_model_path = f\"{BASE_PATH}/models/MGU_Inference_Encoder_CL{context_length}.keras\"\n",
        "        inf_decoder_model_path = f\"{BASE_PATH}/models/MGU_Inference_Decoder_CL{context_length}.keras\"\n",
        "\n",
        "        print(f\"Loading models:\")\n",
        "        print(f\"  - Inference encoder: {inf_encoder_model_path}\")\n",
        "        print(f\"  - Inference decoder: {inf_decoder_model_path}\")\n",
        "\n",
        "        inf_encoder_model = tf.keras.models.load_model(\n",
        "            inf_encoder_model_path,\n",
        "            custom_objects={'MGUCell': MGUCell}\n",
        "        )\n",
        "        inf_decoder_model = tf.keras.models.load_model(\n",
        "            inf_decoder_model_path,\n",
        "            custom_objects={'MGUCell': MGUCell}\n",
        "        )\n",
        "\n",
        "        print(f\"Loading validation dataset...\")\n",
        "        start_time = time.time()\n",
        "        _, val_dataset = load_or_create_datasets(\n",
        "            input_dir=INPUT_DIR,\n",
        "            vocab_dict=vocab_dict,\n",
        "            context_length=context_length,\n",
        "            test_split=TEST_SPLIT,\n",
        "            max_encoder_length=MAX_ENCODER_LENGTH,\n",
        "            max_decoder_length=MAX_DECODER_LENGTH,\n",
        "            dataset_dir=f\"{BASE_PATH}/data_cache\",\n",
        "            batch_size=BATCH_SIZE,\n",
        "            force_regenerate=False\n",
        "        )\n",
        "        data_prep_time = time.time() - start_time\n",
        "        print(f\"Preparation of validation dataset completed in {data_prep_time:.2f} seconds\")\n",
        "\n",
        "        try:\n",
        "          print(f\"Evaluating model...\")\n",
        "          resource_logger.start()\n",
        "          start_time = time.time()\n",
        "          results = evaluate_model(\n",
        "              inf_encoder_model,\n",
        "              inf_decoder_model,\n",
        "              val_dataset,\n",
        "              index_to_word,\n",
        "              vocab_dict,\n",
        "              context_length,\n",
        "              top_k=TOP_K,\n",
        "              max_examples=MAX_EXAMPLES,\n",
        "              max_length=MAX_DECODER_LENGTH\n",
        "          )\n",
        "          eval_time = time.time() - start_time\n",
        "          print(f\"Model evaluation finished in {eval_time:.2f} seconds\")\n",
        "\n",
        "          with open(f'{BASE_PATH}/results/evaluation_results_CL{context_length}.json', 'w') as f:\n",
        "              json.dump(results, f, indent=4)\n",
        "\n",
        "        finally:\n",
        "            resource_logger.stop()\n",
        "            print(\"End of evaluate models block reached\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    train_models()\n",
        "    evaluate_models()\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nExperiment completed in {total_time:.2f} seconds\")\n",
        "    print(f\"Results saved to {BASE_PATH}/results/\")"
      ],
      "metadata": {
        "id": "o6nMkpyY_RTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7a6ede-734a-483c-b4e2-dceb2d0f5638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new vocabulary\n",
            "Building vocabulary using modified tokenization (split on '.' and whitespace)...\n",
            "Counting tokens from: user-ct-test-collection-01.txt\n",
            "Vocabulary Stats:\n",
            "{\n",
            "    \"Requested_Vocabulary_Size\": 45000,\n",
            "    \"Actual_Vocabulary_Size\": 45000,\n",
            "    \"Total_Tokens_Found\": 6464796,\n",
            "    \"Total_Unique_Tokens_Found\": 467801,\n",
            "    \"Coverage_Percentage_Of_Top_Tokens\": 90.04,\n",
            "    \"Special_Tokens\": [\n",
            "        \"<PAD>\",\n",
            "        \"<OOV>\",\n",
            "        \"<START>\",\n",
            "        \"<SEP>\",\n",
            "        \"<END>\"\n",
            "    ]\n",
            "}\n",
            "Vocabulary size: 45000\n",
            "\n",
            "Experiment completed in 5.26 seconds\n",
            "Results saved to /content/drive/MyDrive/MGU/results/\n"
          ]
        }
      ]
    }
  ]
}